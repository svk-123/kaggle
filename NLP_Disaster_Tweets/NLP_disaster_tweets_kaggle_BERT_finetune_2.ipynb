{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b02e4a7-ddd5-4bb1-a20d-f0d22f443ace",
   "metadata": {},
   "source": [
    "##### https://www.kaggle.com/competitions/nlp-getting-started\n",
    "##### Addison Howard, devrishi, Phil Culliton, Yufeng Guo. (2019). Natural Language Processing with Disaster Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0bc04c-62a7-4c05-9cd3-929b87a5e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c83c58-f23f-4aa3-9e4c-ee702c3f7135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset: Disaster: target==1. If no_Disaster: traget==0\n",
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e39ae28-dbad-4115-a16c-ac256038bf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "tmp = df_train.sample(frac=1).reset_index(drop=True)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba741430-d190-4fe8-bfb6-317283cba12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 5)\n",
      "(1613, 5)\n"
     ]
    }
   ],
   "source": [
    "# train val spilit from train-data\n",
    "df_train = tmp[:6000]\n",
    "df_val   = tmp[6000:]\n",
    "df_mini  = tmp[:100]\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dbe08-f9da-4015-ad7a-339906d9e723",
   "metadata": {},
   "source": [
    "##### https://huggingface.co/docs/transformers/model_doc/bert\n",
    "##### https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward.example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4057b14-6c3c-45d0-89f8-8db1989d9578",
   "metadata": {},
   "source": [
    "### Fine-tune BERT using transformers.trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394a54c2-487c-49be-b531-65cb594cf46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                      'bert-base-uncased', \n",
    "                                      num_labels = 2,\n",
    "                                      output_attentions = False,\n",
    "                                      output_hidden_states = False\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcf6faf-d3dd-416b-bc74-f8f5d1c0217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for BERT & Pytorch\n",
    "# Dataset will have: [Input_ids, attention_mask,labels]\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['target']\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466423ed-9333-459a-a302-73e11c443c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "train_dataset = TextDataset(df_train, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TextDataset(df_val, tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "mini_dataset = TextDataset(df_mini, tokenizer)\n",
    "mini_dataloader = DataLoader(mini_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ade5e25-cc2e-4a93-9428-bc642071ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  9619,  2344,  2006, 27686, 14128,  2012, 20116,  2595,  4315,\n",
      "        12502,  3672,  2609,  1011, 20021,  2739, 16074,  8299,  1024,  1013,\n",
      "         1013,  1056,  1012,  2522,  1013, 21025,  2229,  5358,  2290, 26677,\n",
      "         2099,  8299,  1024,  1013,  1013,  1056,  1012,  2522,  1013, 13221,\n",
      "        10343,  2290,  2243,  2546,  2487,  4143,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "#dataset looks something like this\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31588571-f660-4cb8-b29a-5be849b9ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./data',            # Output directory\n",
    "    num_train_epochs=1,                # Number of training epochs\n",
    "    per_device_train_batch_size=32,     # Batch size for training\n",
    "    per_device_eval_batch_size=32,      # Batch size for evaluation\n",
    "    warmup_steps=10,                  # Number of warmup steps\n",
    "    weight_decay=0.001,                 # Strength of weight decay\n",
    "    eval_strategy=\"epoch\",       # Evaluate every epoch\n",
    "    logging_dir='./data',              # Directory for logs\n",
    "    logging_steps=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa578e3e-b5d3-4033-a1a4-7465f57133ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 19:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.367725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=188, training_loss=0.4475020633891542, metrics={'train_runtime': 1202.8555, 'train_samples_per_second': 4.988, 'train_steps_per_second': 0.156, 'total_flos': 394666583040000.0, 'train_loss': 0.4475020633891542, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                        # The fine-tuning model\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=train_dataset,        # Training dataset\n",
    "    eval_dataset=val_dataset           # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72d2ae4-33f3-4ca3-9a9d-6ae45a78858e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/fine_tuned_bert/tokenizer_config.json',\n",
       " './data/fine_tuned_bert/special_tokens_map.json',\n",
       " './data/fine_tuned_bert/vocab.txt',\n",
       " './data/fine_tuned_bert/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Save the fine-tuned model and tokenizer\n",
    "#model.save_pretrained('./data/fine_tuned_bert')\n",
    "#tokenizer.save_pretrained('./data/fine_tuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeab5dac-acfb-45f4-a115-1dd75de10df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def compute_accuracy(model, dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation for faster inference\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get input and labels from the batch\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "\n",
    "            # Move data to the same device as the model\n",
    "            input_ids = input_ids.to(model.device)\n",
    "            attention_mask = attention_mask.to(model.device)\n",
    "            labels = labels.to(model.device)\n",
    "\n",
    "            # Get the model outputs (logits)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get the predicted class by finding the index of the maximum logit (for binary or multi-class classification)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7191ed-eb55-45a3-8fd5-d5204c0e0fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 0.8805\n",
      "Accuracy   Val: 0.8512089274643522\n"
     ]
    }
   ],
   "source": [
    "### Accuracy for train/val samples\n",
    "print('Accuracy Train:',compute_accuracy(model, train_dataloader))\n",
    "print('Accuracy   Val:',compute_accuracy(model, val_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
